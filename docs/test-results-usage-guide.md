# GridTokenX Test Results Usage Guide

This guide explains how to use and interpret the test results generated by the GridTokenX performance and latency testing framework.

## ğŸ“ Test Results Location

All test results are saved in the `./test-results/` directory:

```
test-results/
â”œâ”€â”€ performance/           # Performance test reports
â”‚   â””â”€â”€ performance-report-<timestamp>.json
â””â”€â”€ latency/              # Latency test reports
    â””â”€â”€ demo-latency-report.json
```

## ğŸ” Performance Test Results

### File Structure
Performance test results are saved as JSON files with timestamp:
- **Filename**: `performance-report-<unix-timestamp>.json`
- **Location**: `./test-results/performance/`

### JSON Structure
```json
{
  "timestamp": "2025-11-24T17:41:42.453Z",
  "framework": "GridTokenX Performance Testing",
  "version": "1.0.0",
  "summary": {
    "totalTests": 8,
    "averageLatency": 116.43,
    "averageThroughput": 9.01,
    "averageSuccessRate": 100.00
  },
  "results": [
    {
      "testName": "energy_token_creation",
      "timestamp": "2025-11-24T17:41:22.744Z",
      "iterations": 20,
      "concurrency": 1,
      "totalDuration": 2182,
      "averageLatency": 98.3,
      "minLatency": 51,
      "maxLatency": 148,
      "p95Latency": 148,
      "p99Latency": 148,
      "throughput": 9.17,
      "errorRate": 0,
      "successRate": 100
    }
  ]
}
```

### Key Metrics Explained
- **averageLatency**: Mean response time in milliseconds
- **throughput**: Transactions per second (TPS)
- **p95Latency**: 95th percentile latency (95% of operations complete faster)
- **p99Latency**: 99th percentile latency (99% of operations complete faster)
- **successRate**: Percentage of successful operations

### How to Analyze Performance Results

#### 1. Quick Health Check
```bash
# View latest performance report
cat ./test-results/performance/performance-report-$(ls -t ./test-results/performance/ | head -1 | cut -d'-' -f2-)

# Use jq for pretty parsing
jq '.summary' ./test-results/performance/performance-report-*.json
```

#### 2. Compare Tests Over Time
```bash
# Extract key metrics from multiple reports
for file in ./test-results/performance/*.json; do
  echo "File: $(basename $file)"
  jq -r '.summary | "Avg Latency: \(.averageLatency)ms, Throughput: \(.averageThroughput)TPS"' "$file"
done
```

#### 3. Performance Benchmarking
```javascript
// Node.js script to compare results
const fs = require('fs');
const reports = fs.readdirSync('./test-results/performance/')
  .filter(f => f.endsWith('.json'))
  .map(f => JSON.parse(fs.readFileSync(`./test-results/performance/${f}`)));

// Find best and worst performance
const bestLatency = Math.min(...reports.map(r => r.summary.averageLatency));
const worstLatency = Math.max(...reports.map(r => r.summary.averageLatency));

console.log(`Best latency: ${bestLatency}ms`);
console.log(`Worst latency: ${worstLatency}ms`);
```

## â±ï¸ Latency Test Results

### File Structure
- **Filename**: `demo-latency-report.json`
- **Location**: `./test-results/latency/`

### JSON Structure
```json
{
  "timestamp": "2025-11-24T17:35:15.926Z",
  "framework": "GridTokenX Latency Measurement Framework",
  "version": "1.0.0",
  "summary": {
    "totalMeasurements": 41,
    "averageLatency": 154.61,
    "medianLatency": 154.36,
    "p95": 246.69,
    "p99": 272.38,
    "minLatency": 86.14,
    "maxLatency": 272.38,
    "standardDeviation": 46.79
  },
  "trends": {
    "direction": "stable",
    "confidence": 0.8,
    "slope": 0
  },
  "measurements": [...]
}
```

### Key Metrics Explained
- **totalMeasurements**: Number of individual latency measurements
- **averageLatency**: Mean latency across all measurements
- **medianLatency**: Middle value when sorted by latency
- **standardDeviation**: Measure of latency consistency
- **trends**: Performance trend analysis

### How to Analyze Latency Results

#### 1. Basic Statistics
```bash
# Get summary statistics
jq '.summary' ./test-results/latency/demo-latency-report.json

# Check for performance regressions
if jq '.summary.averageLatency' ./test-results/latency/demo-latency-report.json | grep -E '[2-3][0-9][0-9]\.'; then
  echo "âš ï¸ High latency detected!"
fi
```

#### 2. Latency Distribution Analysis
```javascript
// Analyze latency distribution
const data = JSON.parse(require('fs').readFileSync('./test-results/latency/demo-latency-report.json'));
const measurements = data.measurements.map(m => m.transactionLatency);

// Calculate percentiles manually
const sorted = measurements.sort((a, b) => a - b);
const p50 = sorted[Math.floor(sorted.length * 0.5)];
const p95 = sorted[Math.floor(sorted.length * 0.95)];
const p99 = sorted[Math.floor(sorted.length * 0.99)];

console.log(`P50: ${p50}ms, P95: ${p95}ms, P99: ${p99}ms`);
```

#### 3. Trend Analysis
```bash
# Check performance trends
trend=$(jq -r '.trends.direction' ./test-results/latency/demo-latency-report.json)
confidence=$(jq -r '.trends.confidence' ./test-results/latency/demo-latency-report.json)

echo "Trend: $trend (Confidence: $(echo "$confidence * 100" | bc)%)"

if [[ "$trend" == "degrading" ]]; then
  echo "ğŸš¨ Performance degradation detected!"
elif [[ "$trend" == "improving" ]]; then
  echo "âœ… Performance is improving!"
else
  echo "ğŸ“Š Performance is stable"
fi
```

## ğŸ“Š Integration with CI/CD

### GitHub Actions Example
```yaml
- name: Performance Tests
  run: npm run test:performance-benchmark
  
- name: Upload Performance Results
  uses: actions/upload-artifact@v3
  with:
    name: performance-results
    path: test-results/performance/
    
- name: Performance Regression Check
  run: |
    avg_latency=$(jq '.summary.averageLatency' test-results/performance/*.json)
    if (( $(echo "$avg_latency > 200" | bc -l) )); then
      echo "::error::High latency detected: ${avg_latency}ms"
      exit 1
    fi
```

### Performance Alerts
```bash
# Set up monitoring script
#!/bin/bash
THRESHOLD_LATENCY=200
THRESHOLD_TPS=5

avg_latency=$(jq '.summary.averageLatency' ./test-results/performance/latest.json)
avg_tps=$(jq '.summary.averageThroughput' ./test-results/performance/latest.json)

if (( $(echo "$avg_latency > $THRESHOLD_LATENCY" | bc -l) )); then
  curl -X POST -H "Content-Type: application/json" \
    -d "{\"text\":\"ğŸš¨ High latency: ${avg_latency}ms\"}" \
    $SLACK_WEBHOOK
fi

if (( $(echo "$avg_tps < $THRESHOLD_TPS" | bc -l) )); then
  curl -X POST -H "Content-Type: application/json" \
    -d "{\"text\":\"ğŸ“‰ Low throughput: ${avg_tps} TPS\"}" \
    $SLACK_WEBHOOK
fi
```

## ğŸ› ï¸ Advanced Analysis Tools

### Python Analysis Script
```python
import json
import pandas as pd
import matplotlib.pyplot as plt

# Load performance data
with open('./test-results/performance/latest.json') as f:
    data = json.load(f)

# Convert to DataFrame
df = pd.DataFrame(data['results'])

# Create performance charts
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Latency chart
df.plot(x='testName', y='averageLatency', kind='bar', ax=ax1)
ax1.set_title('Average Latency by Test')
ax1.set_ylabel('Latency (ms)')
ax1.tick_params(axis='x', rotation=45)

# Throughput chart
df.plot(x='testName', y='throughput', kind='bar', ax=ax2, color='green')
ax2.set_title('Throughput by Test')
ax2.set_ylabel('TPS')
ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('./test-results/performance-analysis.png')
```

### Real-time Monitoring Dashboard
```javascript
// Simple dashboard server
const express = require('express');
const fs = require('fs');
const app = express();

app.get('/api/performance', (req, res) => {
  const files = fs.readdirSync('./test-results/performance/');
  const latest = files.sort().pop();
  const data = JSON.parse(fs.readFileSync(`./test-results/performance/${latest}`));
  res.json(data);
});

app.listen(3000, () => {
  console.log('Performance dashboard running on port 3000');
});
```

## ğŸ“ˆ Best Practices

### 1. Regular Testing
- Run performance tests daily: `npm run test:performance-benchmark`
- Run latency tests on changes: `npm run test:latency:demo`
- Use quick checks for rapid feedback: `npm run performance:quick-check`

### 2. Baseline Establishment
```bash
# Establish performance baseline
npm run test:performance-benchmark
cp ./test-results/performance/latest.json ./test-results/performance/baseline.json
```

### 3. Regression Detection
```bash
# Compare against baseline
baseline_avg=$(jq '.summary.averageLatency' ./test-results/performance/baseline.json)
current_avg=$(jq '.summary.averageLatency' ./test-results/performance/latest.json)

regression=$(echo "scale=2; ($current_avg - $baseline_avg) / $baseline_avg * 100" | bc)
echo "Performance change: ${regression}%"

if (( $(echo "$regression > 10" | bc -l) )); then
  echo "ğŸš¨ Performance regression detected!"
fi
```

### 4. Reporting Templates
```markdown
## Performance Report - $(date)

### Summary
- **Average Latency**: $(jq '.summary.averageLatency' latest.json)ms
- **Average Throughput**: $(jq '.summary.averageThroughput' latest.json) TPS
- **Success Rate**: $(jq '.summary.averageSuccessRate' latest.json)%

### Key Findings
- Fastest operation: [extract from data]
- Slowest operation: [extract from data]
- Recommendations: [extract from data]
```

## ğŸ”§ Troubleshooting

### Common Issues

1. **Missing Results Files**
   ```bash
   # Check if results directory exists
   ls -la ./test-results/
   
   # Create if missing
   mkdir -p ./test-results/performance ./test-results/latency
   ```

2. **JSON Parse Errors**
   ```bash
   # Validate JSON syntax
   jq '.' ./test-results/performance/latest.json
   
   # Fix common issues (trailing commas, etc.)
   ```

3. **Permission Issues**
   ```bash
   # Fix file permissions
   chmod 644 ./test-results/performance/*.json
   chmod 755 ./test-results/
   ```

## ğŸ“š Additional Resources

- **Performance Testing Guide**: `docs/performance-testing.md`
- **Latency Measurement Plan**: `docs/latency-measurement-plan.md`
- **Architecture Documentation**: `docs/P2P_ARCHITECTURE.md`

For questions or issues, refer to the test framework source code in:
- `tests/performance/architecture/run-performance-tests.ts`
- `tests/latency/framework/`
